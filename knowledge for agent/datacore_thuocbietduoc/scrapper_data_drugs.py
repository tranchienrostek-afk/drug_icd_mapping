import asyncio
import pandas as pd
import os
import sys
from datetime import datetime
from playwright.async_api import async_playwright

# ================= CONFIG =================
BASE_URL = "https://thuocbietduoc.com.vn/thuoc/drgsearch.aspx"
START_PAGE = 1829
MAX_PAGES = 3424
MAX_WORKERS = 6

# File ch·ª©a k·∫øt qu·∫£
OUTPUT_FILE = f"ketqua_thuoc_part_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
# File ghi nh·∫≠t k√Ω ho·∫°t ƒë·ªông (ƒê·ªÉ b·∫°n xem l·∫°i khi VS Code b·ªã t·∫Øt)
HISTORY_LOG_FILE = "session_history.log"
# File ghi l·ªói chi ti·∫øt
ERROR_LOG_FILE = "error_log_tong.txt"

# ================= LOGGING SYSTEM =================
def logger(message):
    """Ghi log ra c·∫£ m√†n h√¨nh console v√† file l∆∞u tr·ªØ"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    formatted_message = f"[{timestamp}] {message}"
    
    # In ra console
    print(formatted_message)
    
    # Ghi v√†o file history (append mode)
    with open(HISTORY_LOG_FILE, "a", encoding="utf-8") as f:
        f.write(formatted_message + "\n")

# ================= UTILS =================
def log_error_page(page_num, reason):
    with open(ERROR_LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now()}] Trang {page_num}: {reason}\n")
    logger(f"‚ùå L·ªñI t·∫°i trang {page_num}: {reason}")

def save_batch(data_list):
    df = pd.DataFrame(data_list)
    header = not os.path.exists(OUTPUT_FILE)
    df.to_csv(OUTPUT_FILE, mode="a", header=header, index=False, encoding="utf-8-sig")

async def clean_text(page, selector):
    try:
        loc = page.locator(selector)
        if await loc.count() > 0:
            return (await loc.first.inner_text()).strip()
    except:
        pass
    return ""

# ================= DETAIL WORKER =================
async def scrape_detail(context, sem, link):
    async with sem:
        page = await context.new_page()
        try:
            await page.goto(link, timeout=30000, wait_until="domcontentloaded")
            record = {
                "so_dang_ky": await clean_text(page, "xpath=//div[contains(text(),'S·ªë ƒëƒÉng k√Ω')]/following-sibling::div"),
                "ten_thuoc": await clean_text(page, "h1"),
                "hoat_chat": await clean_text(page, ".ingredient-content"),
                "noi_dung_dieu_tri": await clean_text(page, "#chi-dinh") or await clean_text(page, "xpath=//h2[contains(text(),'Ch·ªâ ƒë·ªãnh')]/following-sibling::div"),
                "dang_bao_che": await clean_text(page, "xpath=//div[contains(text(),'D·∫°ng b√†o ch·∫ø')]/following-sibling::div"),
                "danh_muc": await clean_text(page, "xpath=//div[contains(text(),'Nh√≥m thu·ªëc')]/following-sibling::div"),
                "ham_luong": await clean_text(page, "xpath=//div[contains(text(),'H√†m l∆∞·ª£ng')]/following-sibling::div"),
                "url_nguon": link
            }
            return record
        except Exception as e:
            return None
        finally:
            await page.close()

# ================= MAIN =================
async def main():
    logger("üöÄ START ASYNC SCRAPER")
    logger(f"üìÑ OUTPUT FILE: {OUTPUT_FILE}")
    logger(f"üìù LOG FILE: {HISTORY_LOG_FILE}")

    sem = asyncio.Semaphore(MAX_WORKERS)

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=["--disable-gpu", "--no-sandbox", "--disable-dev-shm-usage", "--disable-extensions"]
        )

        context = await browser.new_context(
            ignore_https_errors=True,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )

        async def block(route):
            if route.request.resource_type in ["image", "font", "stylesheet"]:
                await route.abort()
            else:
                await route.continue_()

        page = await context.new_page()
        await page.route("**/*", block)
        page.set_default_timeout(60000)

        current_page = START_PAGE

        while current_page <= MAX_PAGES:
            logger(f"üîç ƒêang qu√©t Trang {current_page}/{MAX_PAGES}...")
            page_url = f"{BASE_URL}?page={current_page}"

            load_success = False
            for attempt in range(3):
                try:
                    await page.goto(page_url, wait_until="domcontentloaded")
                    if await page.locator("xpath=/html/body/main/section[3]/div/div/div/div[2]/div/div/a").count() > 0:
                        load_success = True
                        break
                    await asyncio.sleep(1)
                except:
                    await asyncio.sleep(2)

            if not load_success:
                log_error_page(current_page, "Kh√¥ng load ƒë∆∞·ª£c danh s√°ch sau 3 l·∫ßn th·ª≠")
                current_page += 1
                continue

            try:
                product_locator = page.locator("xpath=/html/body/main/section[3]/div/div/div/div[2]/div/div/a")
                links = []
                count = await product_locator.count()
                for i in range(count):
                    href = await product_locator.nth(i).get_attribute("href")
                    if href: links.append(href)

                logger(f"   -> T√¨m th·∫•y {len(links)} link thu·ªëc. B·∫Øt ƒë·∫ßu t·∫£i chi ti·∫øt...")

                tasks = [scrape_detail(context, sem, link) for link in links]
                results = await asyncio.gather(*tasks)
                batch_data = [r for r in results if r]

                if batch_data:
                    save_batch(batch_data)
                    logger(f"   ‚úÖ ƒê√£ l∆∞u {len(batch_data)}/{len(links)} thu·ªëc v√†o CSV.")
                else:
                    log_error_page(current_page, "Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu chi ti·∫øt (Trang tr·∫Øng)")

            except Exception as e:
                log_error_page(current_page, f"L·ªói th·ª±c thi: {str(e)}")

            current_page += 1

        await browser.close()
        logger("‚úÖ CH∆Ø∆†NG TR√åNH HO√ÄN TH√ÄNH")

if __name__ == "__main__":
    asyncio.run(main())